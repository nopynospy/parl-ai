# Masters in Data Science Capstone Project by Tan Kelvin

ParlAI Dialogue Safety Utility with Emoticons and Internet Slangs Translation

# Main Idea
<img
src="https://raw.githubusercontent.com/nopynospy/parl-ai/colab_branch/flowcharts/chatbotflow.png?token=AUKVVNTX5JXRA6M5CHWUYWDB2E5BU"
raw=true
/>
To create a social media chatbot for countering hate speech, where a classifier first detects if a text is toxic. If it is toxic, reply with the Emphathetic model, else reply with the Tweeter model.

# Research Questions
1.	What are the limitations faced by current state of the art natural language processing tools in handling toxic comments?
2.	How could a safer dialogue utility be designed and implemented?
3.	What are the suitable methods and metrics in evaluating the safety performance of a dialogue utility?

# Classification Parameters

Model: ParlAI Safety Dialogue model
Train ratio: 0.73
Validation ratio: 0.12
Test ratio: 0.15
Split seed: 69
Embedding size: 300
Number of layers: 2
Dropout: 0
Activation: Relu
Optimizer: Adamax
Learning rate: 0.0001
Weight decay: Null

# Classification Result for Davidson et al. (2017) dataset

| Model  | Accuracy | F1 |
| ------------- | ------------- | ------------- |
| Sap et al. (2019)  | 91.9  | 75.15  |
| Xia et al. (2020)  | 90.68  | 76.05  |
| Dialogue Safety  | 89.61  | 89.61  |
| Dialogue Safety + emoji converted  | 89.71  | 89.71  |
| Dialogue Safety + slang converted | 88.94  | 88.94  |
| Dialogue Safety + both converted  | 89.5  | 89.5  |

| Model  | Loss | Weighted F1 |
| ------------- | ------------- | ------------- |
| Dialogue Safety  | 0.5514  | 0.8932  |
| Dialogue Safety + emoji converted  | 0.5495  | 0.8939  |
| Dialogue Safety + slang converted | 0.5780  | 0.8874  |
| Dialogue Safety + both converted  | 0.5567  | 0.8917  |

Note: for this dataset, toxic Tweets were collected specifically. So, there are more toxic tweets than non-toxic ones.

# Classification Result for Founta et al. (2018) dataset

| Model  | Accuracy | F1 |
| ------------- | ------------- | ------------- |
| Sap et al. (2019)  | 81.18  | 66.15  |
| Xia et al. (2020)  | 80.27  | 66.08  |
| Dialogue Safety  | 84.13  | 84.13  |
| Dialogue Safety + emoji converted  | 83.94  | 83.94  |
| Dialogue Safety + slang converted | 83.68  | 83.68  |
| Dialogue Safety + both converted  | 85.08  | 85.08  |

| Model  | Loss | Weighted F1 |
| ------------- | ------------- | ------------- |
| Dialogue Safety  | 0.9642  | 0.8423  |
| Dialogue Safety + emoji converted  | 0.9733  | 0.8406  |
| Dialogue Safety + slang converted | 0.9954  | 0.8380  |
| Dialogue Safety + both converted  | 0.9161  | 0.8515  |

Note: for this dataset, Tweets were randomly created. So, there are more non-toxic tweets than toxic ones.

# Automated Chatbot Evaluation by ChatEval

(Only the default parameters were used for Chatbot models)

These results were generated by the ChatEval platform

| Metric  | Human baseline | JHU ParlAI Twitter | Proposed model |
| ------------- | ------------- | ------------- | ------------- |
| Average sentence length  | 12.135  | 12.305  | 22.735  |
| Distinct 1  | 0.504  | 0.035  | 0.085  |
| Distinct 2  | 0.874  | 0.069  | 0.200  |
| Embedding Greedy Match Score  | N/A  | 0.776  | 0.729  |
| Embedding Extrema Score  | N/A  | 0.569  | 0.538  |
| Average Embedding Score  | N/A  | 0.868  | 0.842  |
| BLEU Score  | N/A  | 0.002  | 0.004  |

# Human Evaluation

Three students from a Masters in Data Science programme ranked the Tweet responses. For each of all the 200 Tweets, they select the best response.

| Human baseline  | JHU Tweeter | Proposed model |
| ------------- | ------------- | ------------- |
| 240  | 214  | 223  |

# Limitation of Study

1. Datasets do not contain latest topics, such as those related to the ongoing pandemic. They were mainly collected during an election in the United States
2. Issues in ChatEval platform at the time of writing. Instead, three students from performed the human evaluation
3. Black box models for ParlAI models make it hard to understand what is behind the scenes. Does 'loss' refer to training loss or validation loss? Tweeter model is also biased towards Libertarian views and not being neutral may not be optimal for responding to toxic messages.
4. Only Internet slangs and emoticons were examined, when social media communication also make use of other linguistic features like memes etc.

# Contribution of Study

1. As seen above, ParlAI Dialogue Safety model performed well in the classification. This is because it was built using Bidirectional Encoder Representations from Transformers (BERT), as opposed to bidirectional LSTM in earlier research. BERT tends to perform better in text classification with longer texts.
2. Using two 'chatbot personas' is effective, just as inspired by Xu et al. (2021). Tweeter and Emphathetic models complemented each other. For example, Tweeter model is biased to have Libertarian views, while Emphathetic is more politically neutral.
3. Emoticons and Internet slangs translation improved toxic message classification result, as seen above.

# Recommendation for Future Work

1. More transparency in codes for hate speech classification and chatbot research
2. Amazon Mechanical Turks mainly cosists of evaluators from the United States, which may cause cultural bias. At the time of this writing, the service is not available in Malaysia. ChatEval Tweeter dataset also mainly set in American context
3. The ability to interpret and use Internet slangs and emoticons, as well as memes, should be an key metric in social media chatbot, since these are frequently used

# Intallation
1. Install Microsoft Visual C++ Redistributable at https://aka.ms/vs/16/release/vc_redist.x64.exe
2. Please make sure you are using python version 3.7 and 64-bit
3. Use 'python -m spacy download en' for spacy en_core_web_sm
4. Install packages with either pipenv or pip install

# Note: For the dataset by Founta et al., please obtain from her at https://github.com/ENCASEH2020/hatespeech-twitter